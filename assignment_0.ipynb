{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nastaran/anaconda3/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/nastaran/anaconda3/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/nastaran/anaconda3/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/nastaran/anaconda3/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LNG\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Supress deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('error', category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training inputs (image as vector)\n",
    "X_train = pd.read_csv(\"data/train_in.csv\", header=None).to_numpy()\n",
    "\n",
    "# Training outputs (number that each vector represents)\n",
    "Y_train = pd.read_csv(\"data/train_out.csv\", header=None).to_numpy()\n",
    "\n",
    "X_test = pd.read_csv(\"data/test_in.csv\", header=None).to_numpy()\n",
    "Y_test = pd.read_csv(\"data/test_out.csv\", header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the center for each cloud c_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_digit_cloud(X, Y, digit):\n",
    "    # Finding all the indexes from Y_train_out related to digit\n",
    "    d_indices = np.where(Y == digit)[0]\n",
    "\n",
    "    # an array of all images related to the current d\n",
    "    return X[d_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores calculated centers\n",
    "# Label corresponds to index, eg. Center_0 = clouds[0]\n",
    "cloud_centers = []\n",
    "\n",
    "for d in range(10):\n",
    "    cloud_d = get_digit_cloud(X_train, Y_train, d)\n",
    "\n",
    "    # Calculating the mean/center of each cloud\n",
    "    center_d = np.mean(cloud_d, axis=0)\n",
    "\n",
    "    # add each mean vector to clouds\n",
    "    cloud_centers.append(center_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating center distances for each digit/label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_matrix = np.full((10,10), np.NaN)\n",
    "\n",
    "for row_index in range(0,10):\n",
    "    for column_index in range(0,10):\n",
    "        if column_index == row_index or not np.isnan(distances_matrix[column_index, row_index]):\n",
    "            continue\n",
    "\n",
    "        c1 = cloud_centers[row_index]\n",
    "        c2 = cloud_centers[column_index]\n",
    "        \n",
    "        distances_matrix[row_index, column_index] = LNG.norm(c1 - c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(distances_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(X, Y, title):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap='Set1', s=5)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_confusion_matrix(cm):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".1f\", cmap=\"Blues\", square=True)\n",
    "    plt.title(\"Confusion Matrix (Count)\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"Actual Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiaze PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "# Fit PCA to data\n",
    "clouds_PCA = pca.fit_transform(X_train)\n",
    "\n",
    "#show data\n",
    "visualize(clouds_PCA, Y_train, \"PCA of 256 Dimensional Data\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize UMAP\n",
    "umap_reducer = umap.UMAP(random_state=42)\n",
    "\n",
    "# Fit UMAP model to data\n",
    "clouds_umap = umap_reducer.fit_transform(X_train)\n",
    "\n",
    "# show data\n",
    "visualize(clouds_umap, Y_train, \"UMAP Projection of 256 Dimensional Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize T-SNE\n",
    "tsne = TSNE(n_components=2, learning_rate='auto', init='random', random_state=42)\n",
    "\n",
    "# Fit TSNE model to data\n",
    "cloud_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "#show data\n",
    "visualize(cloud_tsne, Y_train, \"T-SNE of 256 dimensional space\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Nearest Mean Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_mean_classifier(new_vectors):\n",
    "    Y_predict = []\n",
    "\n",
    "    for v in new_vectors:\n",
    "        distances_to_clouds = [LNG.norm(v-center) for center in cloud_centers]\n",
    "\n",
    "        # Returns the label for the first closest cloud\n",
    "        Y_predict.append(np.argmin(distances_to_clouds))\n",
    "    return Y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train K-Nearest-Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(41)\n",
    "knn.fit(X_train, np.ravel(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing NearestMeanClassifier & KNearestNeighborClassifier accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(X, Y, classifier):\n",
    "    correct = 0\n",
    "\n",
    "    Y_predict = classifier(X)\n",
    "\n",
    "    for y_predict, y in zip(Y_predict, Y):\n",
    "        if y_predict == y:\n",
    "            correct+=1\n",
    "     \n",
    "    accuracy_score(Y, Y_predict)\n",
    "    return round(correct / X.shape[0] * 100, 2)\n",
    "\n",
    "# def get_accuracy(X, Y, classifier):\n",
    "#     return round(accuracy_score(Y, classifier(X)) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accurracy on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_acc = get_accuracy(X_train, Y_train, nearest_mean_classifier)\n",
    "print(f'Nearest Mean accuracy on training set: {nm_acc}%')\n",
    "\n",
    "knn_acc = get_accuracy(X_train, Y_train, knn.predict)\n",
    "print(f'KNN accuracy on training set: {knn_acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accurracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_acc = get_accuracy(X_test, Y_test, nearest_mean_classifier)\n",
    "print(f'Nearest Mean accuracy on test set: {nm_acc}%')\n",
    "\n",
    "knn_acc = get_accuracy(X_test, Y_test, knn.predict)\n",
    "print(f'KNN accuracy on test set: {knn_acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_nmc = nearest_mean_classifier(X_train)\n",
    "Y_predict_knn = knn.predict(X_train)\n",
    "Y_test_predict_nmc = nearest_mean_classifier(X_test)\n",
    "Y_test_predict_knn = knn.predict(X_test)\n",
    "\n",
    "confusion_matrix_nmc = confusion_matrix(Y_train, Y_predict_nmc)\n",
    "confusion_matrix_nmc_test = confusion_matrix(Y_test, Y_test_predict_nmc)\n",
    "\n",
    "confusion_matrix_knn = confusion_matrix(Y_train, Y_predict_knn)\n",
    "confusion_matrix_knn_test = confusion_matrix(Y_test, Y_test_predict_knn)\n",
    "\n",
    "print(f'Nearest Mean confusion matrix on the train sets')\n",
    "visualize_confusion_matrix(confusion_matrix_nmc)\n",
    "print(f'Nearest Mean confusion matrix on the test sets')\n",
    "visualize_confusion_matrix(confusion_matrix_nmc_test)\n",
    "\n",
    "print(f'KNN confusion matrix on the train sets')\n",
    "visualize_confusion_matrix(confusion_matrix_knn)\n",
    "print(f'KNN confusion matrix on the test sets')\n",
    "visualize_confusion_matrix(confusion_matrix_knn_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](delta-rule1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide page 13 lecture 2\n",
    "# sigmoid function: ϕ(x)=1/(1+exp(-x))\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# ϕ(x)=1/(1+exp(-x)) we have: ϕʹ(x)=ϕ(x)(1-ϕ(x))=output(1-output)\n",
    "# so once we know ϕ(x) we also know ϕʹ(x) !\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# initialize random weights, multiply by 0.01 to get small values for improving learning rate\n",
    "weights = np.random.randn(257, 10) * 0.1\n",
    "\n",
    "\n",
    "def perceptron(input, labels, alpha, epochs):\n",
    "    np.random.seed(42) \n",
    "    w = weights\n",
    "    for epoch in range(epochs):\n",
    "        for key in range(input.shape[0]):\n",
    "            output = np.dot(input[key], w)\n",
    "            output_sigmoid = sigmoid(output)\n",
    "\n",
    "            target = np.zeros(10)\n",
    "            target[labels[key]] = 1\n",
    "\n",
    "            error = sigmoid_derivative(\n",
    "                output_sigmoid) * (target - output_sigmoid)\n",
    "\n",
    "            # update weights\n",
    "            w = w + alpha * np.outer(input[key], error)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # Compute accuracy for training set\n",
    "            output_training = np.dot(input, w)\n",
    "            predictions = np.argmax(sigmoid(output_training), axis=1)\n",
    "            accuracy = np.mean(predictions == labels)\n",
    "            print(f\"Epoch {epoch}, Training Set Accuracy: {accuracy}\")\n",
    "\n",
    "            # Compute accuracy for test set\n",
    "            output_test = np.dot(np.hstack([X_test, np.ones((X_test.shape[0], 1))]), w)\n",
    "            predictions = np.argmax(sigmoid(output_test), axis=1)\n",
    "            accuracy = np.mean(predictions == Y_test.flatten())\n",
    "            print(f\"Epoch {epoch}, Test Set Accuracy: {accuracy} \\n****************************************\")\n",
    "\n",
    "alpha = 0.01\n",
    "epochs = 100\n",
    "\n",
    "perceptron(\n",
    "    # add bias to input\n",
    "    np.hstack([X_train, np.ones((X_train.shape[0], 1))]),\n",
    "    Y_train.flatten(),\n",
    "    alpha,\n",
    "    epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, debug=False) -> None:\n",
    "        self.weights = np.random.uniform(-1,1,(input_size, output_size))\n",
    "        self.biases = np.ones((1,output_size))  # Initialize biases\n",
    "        self.debug = debug\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, a):\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def activate(self, Z):\n",
    "        return np.apply_along_axis(self.sigmoid, 0, Z)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.inputs = input_data\n",
    "        # z\n",
    "        self.z = np.dot(input_data, self.weights) + self.biases\n",
    "        # a\n",
    "        self.output = self.activate(self.z)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_error):\n",
    "        # TODO\n",
    "        d_activation = sigmoid_derivative(self.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "Y_train = np.array([\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0\n",
    "])\n",
    "\n",
    "# Network architecture\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100 * 100\n",
    "\n",
    "#initialize weights and biases\n",
    "#layer 1 \n",
    "w1 = np.random.uniform(-1,1,(input_size, hidden_size))\n",
    "b1 = np.zeros((1,hidden_size))\n",
    "#layer2\n",
    "w2 = np.random.uniform(-1,1,(hidden_size, output_size))\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(epochs):\n",
    "    # First layer output\n",
    "    z1 = np.dot(X_train, w1) + b1\n",
    "\n",
    "    # Apply sigmoid activation\n",
    "    a1 = 1 / (1 + np.exp(-z1))\n",
    "\n",
    "    # Second layer ouput\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "\n",
    "    #Apply sigmoid activation\n",
    "    a2 = 1 / (1 + np.exp(-z2))\n",
    "\n",
    "    # Calculate loss (MSE)\n",
    "    loss = np.mean((a2 - Y_train)**2)\n",
    "\n",
    "    #Backward pass\n",
    "    d_mse = 2 * (a2 - Y_train[:, np.newaxis])\n",
    "\n",
    "    d_a2 = a2 * (1 - a2)\n",
    "    d_z2 = a1\n",
    "    \n",
    "    d_w2 = np.dot(d_z2.T, d_mse * d_a2)\n",
    "    d_b2 = np.sum(d_mse * d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = a1 * (1 - a1)\n",
    "    d_z1 = X_train\n",
    "\n",
    "    d_w1 = np.dot(d_z1.T, np.dot(d_mse * d_a2, w2.T) * d_a1)\n",
    "    d_b1 = np.sum(np.dot(d_mse * d_a2, w2.T) * d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update weights and biases\n",
    "    w1 -= learning_rate * d_w1\n",
    "    b1 -= learning_rate * d_b1\n",
    "    w2 -= learning_rate * d_w2\n",
    "    b2 -= learning_rate * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([0,0])\n",
    "\n",
    "# First layer output\n",
    "z1 = np.dot(X, w1) + b1\n",
    "\n",
    "# Apply sigmoid activation\n",
    "a1 = 1 / (1 + np.exp(-z1))\n",
    "\n",
    "# Second layer ouput\n",
    "z2 = np.dot(a1, w2) + b2\n",
    "\n",
    "a2 = 1 / (1 + np.exp(-z2))\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.19181376423688556\n",
      "Epoch: 200, Loss: 0.16133105974764594\n",
      "Epoch: 300, Loss: 0.1298159932800757\n",
      "\n",
      "Predictions\n",
      "Input: [0 0], True Label: 0, Predicted: 1\n",
      "Input: [0 1], True Label: 1, Predicted: 1\n",
      "Input: [1 0], True Label: 1, Predicted: 1\n",
      "Input: [1 1], True Label: 0, Predicted: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "X_train = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "def mse(Y_pred, Y_true):\n",
    "    return np.mean(np.square(Y_pred - Y_true))\n",
    "\n",
    "# Hyperbolic Tangent\n",
    "def tanh_activation(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(a):\n",
    "    return 1.0 - a**2\n",
    "    \n",
    "def predict(w1, w2, b1, b2):\n",
    "        z1 = np.dot(X_train, w1) + b1\n",
    "        a1 = tanh_activation(z1)\n",
    "        z2 = np.dot(a1, w2) + b2\n",
    "        a2 = tanh_activation(z2)\n",
    "\n",
    "        return z1, a1, z2, a2\n",
    "\n",
    "def unpack_weights(weights):\n",
    "    w1 = weights[:4].reshape(2, 2)\n",
    "    b1 = weights[4:6].reshape(1, 2)\n",
    "    w2 = weights[6:8].reshape(2, 1)\n",
    "    b2 = weights[8].reshape(1, 1)\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def grdmse(weights, epochs=1000, alpha=0.1, max_loss = 0.1):\n",
    "    \n",
    "    epoch = 0\n",
    "    loss = 1\n",
    "    while epoch < epochs and loss > max_loss:\n",
    "        w1, b1, w2, b2 = unpack_weights(weights)\n",
    "        # Feedforward\n",
    "        z1, a1, z2, a2 = predict(w1, w2, b1, b2)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss =  mse(a2, Y_train)\n",
    "\n",
    "        # Backpropagation\n",
    "        dJ_da2 = a2 - Y_train\n",
    "        da2_dz2 = tanh_derivative(a2)\n",
    "        dz2_dw2 = a1\n",
    "        dJ_dw2 = np.dot(dz2_dw2.T, dJ_da2 * da2_dz2)\n",
    "\n",
    "        dz2_da1 = w2\n",
    "        \n",
    "        dJ_da1 = np.dot(dJ_da2 * da2_dz2, dz2_da1.T)\n",
    "        dJ_dw1 = np.dot(X_train.T, dJ_da1 * tanh_derivative(a1))\n",
    "\n",
    "        w1 -= alpha * dJ_dw1\n",
    "        w2 -= alpha * dJ_dw2\n",
    "        b1 -= alpha * np.sum(dJ_da1 * tanh_derivative(a1), axis=0)\n",
    "        b2 -= alpha * np.sum(dJ_da2 * da2_dz2, axis=0)\n",
    "\n",
    "        epoch += 1\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss}')\n",
    "\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "\n",
    "weights = np.random.uniform(-1, 1, (9))\n",
    "\n",
    "w1, b1, w2, b2 = grdmse(weights)\n",
    "\n",
    "# Predictions\n",
    "print(\"\\nPredictions\")\n",
    "for x, y in zip(X_train, Y_train):\n",
    "    z1 = np.dot(x, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    print(f\"Input: {x}, True Label: {y[0]}, Predicted: {round(a2[0][0])}\")\n",
    "\n",
    "z1, a1, z2, a2 = predict(w1, w2, b1, b2)\n",
    "Y_pred = np.where(a2 < 0.5, 0, 1)\n",
    "accuracy_score(Y_train, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_idl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
